import cv2
import numpy as np
import torch
import requests
import json
import re
import base64
import textwrap
import queue
import time
import io
import sounddevice as sd
from scipy.io.wavfile import write
from pydub import AudioSegment
from ultralytics.models.sam import Predictor as SAMPredictor


# ----------------------- åŸºç¡€å·¥å…·å‡½æ•° -----------------------

def encode_np_array(image_np):
    """å°† numpy å›¾åƒæ•°ç»„ï¼ˆBGRï¼‰ç¼–ç ä¸º base64 å­—ç¬¦ä¸²"""
    success, buffer = cv2.imencode('.jpg', image_np)
    if not success:
        raise ValueError("æ— æ³•å°†å›¾åƒæ•°ç»„ç¼–ç ä¸º JPEG")
    img_base64 = base64.b64encode(buffer).decode('utf-8')
    return img_base64


def plot_coordinates(image_input, coords):
    """æ ¹æ®åæ ‡æ‰“å°ç®€å•æç¤ºï¼Œä½†ä¸æ˜¾ç¤ºå›¾åƒ"""
    if not coords:
        print("[æç¤º] æœªæå–åˆ°åæ ‡ä¿¡æ¯ã€‚")
        return
    if "bbox" in coords:
        print(f"[æ ‡æ³¨] è¾¹ç•Œæ¡†åæ ‡: {coords['bbox']}")
    elif "point" in coords:
        print(f"[æ ‡æ³¨] ä¸­å¿ƒç‚¹åæ ‡: {coords['point']}")
    elif "x" in coords and "y" in coords:
        print(f"[æ ‡æ³¨] ä¸­å¿ƒç‚¹åæ ‡: ({coords['x']}, {coords['y']})")


# ----------------------- å¤šæ¨¡æ€æ¨¡å‹è°ƒç”¨ï¼ˆQwenï¼‰ -----------------------

def generate_robot_actions(user_command, image_input=None):
    """
    ä½¿ç”¨ base64 çš„æ–¹å¼å°† numpy å›¾åƒå’Œç”¨æˆ·æ–‡æœ¬æŒ‡ä»¤ä¼ ç»™ Qwen å¤šæ¨¡æ€æ¨¡å‹ï¼Œ
    è¦æ±‚æ¨¡å‹è¿”å›ä¸¤éƒ¨åˆ†ï¼š
      - æ¨¡å‹è¿”å›å†…å®¹ä¸­ï¼Œç¬¬ä¸€éƒ¨åˆ†ä¸ºè‡ªç„¶è¯­è¨€å“åº”ï¼ˆè¯´æ˜ä¸ºä½•é€‰æ‹©è¯¥ç‰©ä½“ï¼‰ï¼Œ
      - ç´§è·Ÿå…¶åçš„éƒ¨åˆ†ä¸ºçº¯ JSON å¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

        {
          "name": "ç‰©ä½“åç§°",
          "bbox": [å·¦ä¸Šè§’x, å·¦ä¸Šè§’y, å³ä¸‹è§’x, å³ä¸‹è§’y]
        }

    è¿”å›ä¸€ä¸ª dictï¼ŒåŒ…å« "response" å’Œ "coordinates"ã€‚
    å‚æ•° image_input ä¸º numpy æ•°ç»„ï¼ˆBGR æ ¼å¼ï¼‰ã€‚
    """
    url = "http://192.168.1.6:1034/v1/chat/completions"
    headers = {"Content-Type": "application/json"}

    system_prompt = textwrap.dedent("""\
    ä½ æ˜¯ä¸€ä¸ªç²¾å¯†æœºæ¢°è‡‚è§†è§‰æ§åˆ¶ç³»ç»Ÿï¼Œå…·å¤‡å…ˆè¿›çš„å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œä»»åŠ¡ï¼š

    ã€å›¾åƒåˆ†æé˜¶æ®µã€‘
    1. åˆ†æè¾“å…¥å›¾åƒï¼Œè¯†åˆ«å›¾åƒä¸­æ‰€æœ‰å¯è§ç‰©ä½“ï¼Œå¹¶è®°å½•æ¯ä¸ªç‰©ä½“çš„è¾¹ç•Œæ¡†ï¼ˆå·¦ä¸Šè§’ç‚¹å’Œå³ä¸‹è§’ç‚¹ï¼‰åŠå…¶ç±»åˆ«åç§°ã€‚

    ã€æŒ‡ä»¤è§£æé˜¶æ®µã€‘
    2. æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œä»è¯†åˆ«çš„ç‰©ä½“ä¸­ç­›é€‰å‡ºæœ€åŒ¹é…çš„ç›®æ ‡ç‰©ä½“ã€‚

    ã€å“åº”ç”Ÿæˆé˜¶æ®µã€‘
    3. è¾“å‡ºæ ¼å¼å¿…é¡»ä¸¥æ ¼å¦‚ä¸‹ï¼š
    - è‡ªç„¶è¯­è¨€å“åº”ï¼ˆä»…åŒ…å«è¯´æ˜ä¸ºä½•é€‰æ‹©è¯¥ç‰©ä½“çš„æ–‡å­—,å¯ä»¥ä¿çš®å¯çˆ±åœ°å›åº”ç”¨æˆ·çš„éœ€æ±‚ï¼Œä½†æ˜¯è¯·æ³¨æ„ï¼Œå›ç­”ä¸­åº”è¯¥åªåŒ…å«è¢«é€‰ä¸­çš„ç‰©ä½“ï¼‰ï¼Œ
    - ç´§è·Ÿå…¶åï¼Œä»ä¸‹ä¸€è¡Œå¼€å§‹è¿”å› **æ ‡å‡† JSON å¯¹è±¡**ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

    {
      "name": "ç‰©ä½“åç§°",
      "bbox": [å·¦ä¸Šè§’x, å·¦ä¸Šè§’y, å³ä¸‹è§’x, å³ä¸‹è§’y]
    }

    ã€æ³¨æ„äº‹é¡¹ã€‘
    - JSON å¿…é¡»ä»ä¸‹ä¸€è¡Œå¼€å§‹ï¼›
    - è‡ªç„¶è¯­è¨€å“åº”ä¸ JSON ä¹‹é—´æ— å…¶ä»–é¢å¤–æ–‡æœ¬ï¼›
    - JSON å¯¹è±¡ä¸èƒ½æœ‰æ³¨é‡Šã€é¢å¤–æ–‡æœ¬æˆ–è§£é‡Šï¼›
    - åæ ‡ bbox å¿…é¡»ä¸ºæ•´æ•°ï¼›
    - åªå…è®¸ä½¿ç”¨ "bbox" ä½œä¸ºåæ ‡æ ¼å¼ã€‚
    """)

    messages = [{"role": "system", "content": system_prompt}]
    user_content = []

    if image_input is not None:
        base64_img = encode_np_array(image_input)
        user_content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{base64_img}"
            }
        })

    user_content.append({"type": "text", "text": user_command})
    messages.append({"role": "user", "content": user_content})

    data = {
        "model": "qwen7b",
        "messages": messages
    }

    try:
        response = requests.post(url, headers=headers, json=data, timeout=15)
        response.raise_for_status()
        content = response.json()["choices"][0]["message"]["content"]
        print("åŸå§‹å“åº”ï¼š", content)

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ JSON éƒ¨åˆ†
        match = re.search(r'(\{.*\})', content, re.DOTALL)
        if match:
            json_str = match.group(1)
            try:
                coord = json.loads(json_str)
            except Exception as e:
                print(f"[è­¦å‘Š] JSON è§£æå¤±è´¥ï¼š{e}")
                coord = {}
            natural_response = content[:match.start()].strip()
        else:
            natural_response = content.strip()
            coord = {}

        return {
            "response": natural_response,
            "coordinates": coord
        }

    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥ï¼š{e}")
        return {"response": "å¤„ç†å¤±è´¥", "coordinates": {}}


# ----------------------- SAM åˆ†å‰²ç›¸å…³ -----------------------

def choose_model():
    """åˆå§‹åŒ– SAM åˆ†å‰²é¢„æµ‹å™¨ï¼Œè®¾ç½®ç›¸å…³å‚æ•°"""
    model_weight = 'sam_b.pt'
    overrides = dict(
        task='segment',
        mode='predict',
        model=model_weight,
        conf=0.01,
        save=False
    )
    return SAMPredictor(overrides=overrides)


def process_sam_results(results):
    """å¤„ç† SAM åˆ†å‰²ç»“æœï¼Œè·å–æ©ç å’Œä¸­å¿ƒç‚¹"""
    if not results or not results[0].masks:
        return None, None
    mask = results[0].masks.data[0].cpu().numpy()
    mask = (mask > 0).astype(np.uint8) * 255
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None, mask
    M = cv2.moments(contours[0])
    if M["m00"] == 0:
        return None, mask
    cx = int(M["m10"] / M["m00"])
    cy = int(M["m01"] / M["m00"])
    return (cx, cy), mask


# ----------------------- è¯­éŸ³è¯†åˆ«ä¸ TTS -----------------------

samplerate = 16000
channels = 1
dtype = 'int16'
frame_duration = 0.2
frame_samples = int(frame_duration * samplerate)
silence_threshold = 500
silence_max_duration = 1.0
q = queue.Queue()


def rms(audio_frame):
    samples = np.frombuffer(audio_frame, dtype=np.int16)
    if samples.size == 0:
        return 0
    mean_square = np.mean(samples.astype(np.float32) ** 2)
    if np.isnan(mean_square) or mean_square < 1e-5:
        return 0
    return np.sqrt(mean_square)


def callback(indata, frames, time_info, status):
    if status:
        print("âš ï¸ çŠ¶æ€è­¦å‘Šï¼š", status)
    q.put(bytes(indata))


def recognize_speech():
    """å½•éŸ³å¹¶è¿”å›éŸ³é¢‘æ•°æ®ï¼ˆnumpy æ•°ç»„ï¼‰"""
    print("ğŸ™ï¸ å¯åŠ¨å½•éŸ³ï¼Œè¯·è¯´è¯...")
    audio_buffer = []
    is_speaking = False
    last_voice_time = time.time()

    with sd.RawInputStream(samplerate=samplerate, blocksize=frame_samples,
                           dtype=dtype, channels=channels, callback=callback):
        while True:
            frame = q.get()
            volume = rms(frame)
            current_time = time.time()
            if volume > silence_threshold:
                if not is_speaking:
                    print("ğŸ¤ æ£€æµ‹åˆ°è¯­éŸ³ï¼Œå¼€å§‹å½•éŸ³...")
                    is_speaking = True
                    audio_buffer = []
                audio_np = np.frombuffer(frame, dtype=np.int16)
                audio_buffer.append(audio_np)
                last_voice_time = current_time
            elif is_speaking and (current_time - last_voice_time > silence_max_duration):
                print("ğŸ›‘ åœæ­¢å½•éŸ³ï¼Œå‡†å¤‡è¯†åˆ«...")
                full_audio = np.concatenate(audio_buffer, axis=0)
                return full_audio


def speech_to_text(audio_data):
    """
    å°†å½•éŸ³æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬ï¼š
    å…ˆå°† numpy æ•°ç»„ä¿å­˜ä¸º wavï¼Œå†è½¬æ¢ä¸º mp3 å‘é€è‡³ ASR æ¥å£ã€‚
    """
    ASR_API_URL = "http://192.168.1.6:3003/v1/audio/transcriptions"
    ASR_API_TOKEN = "tts-ncut1034"

    wav_io = io.BytesIO()
    write(wav_io, samplerate, audio_data.astype(np.int16))
    wav_io.seek(0)
    audio = AudioSegment.from_wav(wav_io)
    mp3_io = io.BytesIO()
    audio.export(mp3_io, format="mp3")
    mp3_io.seek(0)
    headers = {"Authorization": f"Bearer {ASR_API_TOKEN}"}
    files = {"file": ("speech.mp3", mp3_io, "audio/mpeg")}
    print("ğŸ“¡ æ­£åœ¨è¯†åˆ«è¯­éŸ³...")
    response = requests.post(ASR_API_URL, headers=headers, files=files)
    if response.ok:
        return response.text.strip()
    else:
        print("âŒ è¯­éŸ³è¯†åˆ«æ¥å£å¤±è´¥ï¼š", response.status_code)
        return ""


def play_tts(text):
    """
    è°ƒç”¨ TTS æ¥å£ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³å¹¶æ’­æ”¾
    """
    TTS_API_URL = "http://192.168.1.6:3002/v1/audio/speech"
    TTS_API_TOKEN = "tts-ncut1034"
    print("ğŸ“¢ æ’­æ”¾ TTS è¯­éŸ³ï¼š", text)
    payload = {
        "model": "cosyvoice",
        "voice": "tarzan",
        "input": text,
        "response_format": "mp3",
        "speed": 1.2
    }
    headers = {
        "Authorization": f"Bearer {TTS_API_TOKEN}",
        "Content-Type": "application/json"
    }
    try:
        response = requests.post(TTS_API_URL, headers=headers, data=json.dumps(payload))
        if response.ok:
            audio_data = response.content
            audio_io = io.BytesIO(audio_data)
            audio_seg = AudioSegment.from_file(audio_io, format="mp3")
            raw = np.array(audio_seg.get_array_of_samples())
            raw = raw.reshape((-1, audio_seg.channels))
            sd.play(raw, audio_seg.frame_rate)
            sd.wait()
        else:
            print("âŒ TTS æ¥å£å¤±è´¥ï¼š", response.status_code)
    except Exception as e:
        print("âŒ æ’­æŠ¥å¤±è´¥ï¼š", e)


def voice_command_to_keyword():
    """
    è·å–è¯­éŸ³å‘½ä»¤å¹¶è½¬æ¢ä¸ºæ–‡æœ¬ã€‚
    ç›´æ¥è¿”å›è¯†åˆ«çš„æ–‡æœ¬æŒ‡ä»¤ã€‚
    """
    audio_data = recognize_speech()
    text = speech_to_text(audio_data)
    if not text:
        print("âš ï¸ æ²¡æœ‰è¯†åˆ«åˆ°æ–‡æœ¬")
        return ""
    print("ğŸ“ è¯†åˆ«æ–‡æœ¬ï¼š", text)
    return text


# ----------------------- ä¸»æµç¨‹ï¼šå›¾åƒåˆ†å‰² -----------------------

def segment_image(image_input, output_mask='mask1.png'):
    """
    è‡ªåŠ¨è¯­éŸ³è·å–æ£€æµ‹ç›®æ ‡ â†’ å¤šæ¨¡æ€æ¨¡å‹æ£€æµ‹ â†’ SAM åˆ†å‰² â†’ ä¿å­˜æ©ç 
    å‚æ•° image_input ä¸º numpy æ•°ç»„ï¼ˆBGR æ ¼å¼ï¼‰ã€‚
    æ£€æµ‹ä¸åˆ°æ—¶æ”¯æŒæ‰‹åŠ¨ç‚¹å‡»é€‰æ‹©ç›®æ ‡åŒºåŸŸã€‚
    """
    # 1. ä½¿ç”¨è¯­éŸ³è·å–ç›®æ ‡æŒ‡ä»¤
    print("ğŸ™ï¸ è¯·é€šè¿‡è¯­éŸ³æè¿°ç›®æ ‡ç‰©ä½“åŠæŠ“å–æŒ‡ä»¤...")
    command_text = voice_command_to_keyword()
    if not command_text:
        print("âš ï¸ æœªè¯†åˆ«åˆ°è¯­éŸ³æŒ‡ä»¤ï¼Œè¯·é‡è¯•ã€‚")
        return None
    print(f"âœ… è¯†åˆ«çš„è¯­éŸ³æŒ‡ä»¤ï¼š{command_text}")

    # 2. é€šè¿‡å¤šæ¨¡æ€æ¨¡å‹è·å–æ£€æµ‹æ¡†
    result = generate_robot_actions(command_text, image_input)
    natural_response = result["response"]
    detection_info = result["coordinates"]
    print("è‡ªç„¶è¯­è¨€å›åº”ï¼š", natural_response)
    print("æ£€æµ‹åˆ°çš„ç‰©ä½“ä¿¡æ¯ï¼š", detection_info)

    # ä»…å¯¹æ¨¡å‹è¿”å›çš„è‡ªç„¶è¯­è¨€å›åº”æ’­æŠ¥
    play_tts(natural_response)

    bbox = detection_info.get("bbox") if detection_info and "bbox" in detection_info else None

    # 3. å‡†å¤‡å›¾åƒä¾› SAM ä½¿ç”¨ï¼ˆè½¬æ¢ä¸º RGBï¼‰
    image_rgb = cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB)

    # 4. åˆå§‹åŒ– SAMï¼Œå¹¶è®¾ç½®å›¾åƒ
    predictor = choose_model()
    predictor.set_image(image_rgb)

    if bbox:
        results = predictor(bboxes=[bbox])
        center, mask = process_sam_results(results)
        print(f"âœ… è‡ªåŠ¨æ£€æµ‹åˆ°ç›®æ ‡ï¼Œbboxï¼š{bbox}")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œè¯·ç‚¹å‡»å›¾åƒé€‰æ‹©å¯¹è±¡")
        cv2.namedWindow('Select Object', cv2.WINDOW_NORMAL)
        cv2.imshow('Select Object', image_input)
        point = []

        def click_handler(event, x, y, flags, param):
            if event == cv2.EVENT_LBUTTONDOWN:
                point.extend([x, y])
                print(f"ğŸ–±ï¸ ç‚¹å‡»åæ ‡ï¼š{x}, {y}")
                cv2.setMouseCallback('Select Object', lambda *args: None)

        cv2.setMouseCallback('Select Object', click_handler)
        while True:
            key = cv2.waitKey(100)
            if point:
                break
            if cv2.getWindowProperty('Select Object', cv2.WND_PROP_VISIBLE) < 1:
                print("âŒ çª—å£è¢«å…³é—­ï¼Œæœªè¿›è¡Œç‚¹å‡»")
                return None
        cv2.destroyAllWindows()
        results = predictor(points=[point], labels=[1])
        center, mask = process_sam_results(results)

    # 5. ä¿å­˜åˆ†å‰²æ©ç 
    if mask is not None:
        cv2.imwrite(output_mask, mask, [cv2.IMWRITE_PNG_BILEVEL, 1])
        print(f"âœ… åˆ†å‰²æ©ç å·²ä¿å­˜ï¼š{output_mask}")
    else:
        print("âš ï¸ åˆ†å‰²å¤±è´¥ï¼Œæœªç”Ÿæˆæ©ç ")
    return mask


# ----------------------- ä¸»ç¨‹åºå…¥å£ -----------------------

if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šä½¿ç”¨ cv2 è¯»å–å›¾åƒï¼Œå¹¶ä»¥ numpy æ•°ç»„ä¼ å…¥
    input_image = cv2.imread('color.png')
    if input_image is None:
        raise ValueError("æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶: color.png")
    seg_mask = segment_image(input_image)
    if seg_mask is not None:
        print("Segmentation result mask shape:", seg_mask.shape)
    else:
        print("Segmentation result: None")
