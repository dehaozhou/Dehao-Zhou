import cv2
import numpy as np
import torch
from ultralytics import YOLO
from ultralytics.models.sam import Predictor as SAMPredictor

def choose_model():
    """Initialize SAM predictor with proper parameters"""
    model_weight = 'sam_b.pt'
    overrides = dict(
        task='segment',
        mode='predict',
        #imgsz=1024,
        model=model_weight,
        conf=0.01,
        save=False
    )
    return SAMPredictor(overrides=overrides)


def set_classes(model, target_class):
    """Set YOLO-World model to detect specific class"""
    model.set_classes([target_class])


def detect_objects(image_or_path, target_class=None):
    """
    Detect objects with YOLO-World
    image_or_path: can be a file path (str) or a numpy array (image).
    Returns: (list of bboxes in xyxy format, detected classes list, visualization image)
    """
    model = YOLO("yolov8s-world.pt")
    if target_class:
        set_classes(model, target_class)

    # YOLOv8 çš„ predict å¯åŒæ—¶å¤„ç† æ–‡ä»¶è·¯å¾„(str) æˆ– å›¾åƒæ•°ç»„(np.ndarray)
    results = model.predict(image_or_path)

    boxes = results[0].boxes
    vis_img = results[0].plot()  # Get visualized detection results

    # Extract valid detections
    valid_boxes = []
    for box in boxes:
        if box.conf.item() > 0.25:  # Confidence threshold
            valid_boxes.append({
                "xyxy": box.xyxy[0].tolist(),
                "conf": box.conf.item(),
                "cls": results[0].names[box.cls.item()]
            })

    return valid_boxes, vis_img


def process_sam_results(results):
    """Process SAM results to get mask and center point"""
    if not results or not results[0].masks:
        return None, None

    # Get first mask (assuming single object segmentation)
    mask = results[0].masks.data[0].cpu().numpy()
    mask = (mask > 0).astype(np.uint8) * 255

    # Find contour and center
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None, None

    M = cv2.moments(contours[0])
    if M["m00"] == 0:
        return None, mask

    cx = int(M["m10"] / M["m00"])
    cy = int(M["m01"] / M["m00"])
    return (cx, cy), mask


from listen import voice_command_to_keyword  # âœ… å¯¼å…¥è¯­éŸ³è¯†åˆ«å‡½æ•°

def segment_image(image_path, output_mask='mask1.png'):
    """
    image_path: å›¾åƒè·¯å¾„æˆ– BGR å›¾åƒæ•°ç»„
    output_mask: æ©ç ä¿å­˜è·¯å¾„
    è‡ªåŠ¨è¯­éŸ³è·å–æ£€æµ‹ç›®æ ‡ â†’ YOLO æ£€æµ‹ â†’ SAM åˆ†å‰² â†’ ä¿å­˜æ©ç 
    æ£€æµ‹ä¸åˆ°æ—¶æ”¯æŒæ‰‹åŠ¨ç‚¹å‡»ç›®æ ‡ã€‚
    """

    # âœ… ä½¿ç”¨è¯­éŸ³è·å–ç›®æ ‡ç±»åˆ«
    print("ğŸ™ï¸ è¯·é€šè¿‡è¯­éŸ³è¾“å…¥ç›®æ ‡ç‰©ä½“åç§°...")
    target_class = voice_command_to_keyword()
    if not target_class:
        print("âš ï¸ æœªè¯†åˆ«å‡ºç›®æ ‡ç±»åˆ«ï¼Œè¯·é‡è¯•ã€‚")
        return None
    print(f"âœ… è¯­éŸ³è¯†åˆ«ç›®æ ‡ç±»åˆ«ä¸ºï¼š{target_class}")

    # 2) ä½¿ç”¨ YOLO æ£€æµ‹
    detections, vis_img = detect_objects(image_path, target_class)

    # ä¿å­˜æ£€æµ‹å›¾
    cv2.imwrite('detection_visualization.jpg', vis_img)

    # 3) å‡†å¤‡ RGB å›¾åƒç”¨äº SAM
    if isinstance(image_path, str):
        bgr_img = cv2.imread(image_path)
        if bgr_img is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾åƒè·¯å¾„: {image_path}")
        image_rgb = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)
    else:
        image_rgb = cv2.cvtColor(image_path, cv2.COLOR_BGR2RGB)

    # 4) åˆå§‹åŒ– SAM
    predictor = choose_model()
    predictor.set_image(image_rgb)

    # 5) åˆ¤æ–­æ˜¯å¦æ£€æµ‹åˆ°ç›®æ ‡
    if detections:
        best_det = max(detections, key=lambda x: x["conf"])
        results = predictor(bboxes=[best_det["xyxy"]])
        center, mask = process_sam_results(results)
        print(f"âœ… è‡ªåŠ¨æ£€æµ‹åˆ°ç›®æ ‡ï¼š{best_det['cls']}ï¼Œç½®ä¿¡åº¦ï¼š{best_det['conf']:.2f}")
    else:
        # ğŸ–±ï¸ æ‰‹åŠ¨ç‚¹å‡»é€‰æ‹©
        print("âš ï¸ æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œè¯·ç‚¹å‡»å›¾åƒé€‰æ‹©å¯¹è±¡")

        point = []

        def click_handler(event, x, y, flags, param):
            if event == cv2.EVENT_LBUTTONDOWN:
                point.extend([x, y])
                print(f"ğŸ–±ï¸ ç‚¹å‡»åæ ‡ï¼š{x}, {y}")
                cv2.setMouseCallback('Select Object', lambda *args: None)

        cv2.namedWindow('Select Object', cv2.WINDOW_NORMAL)
        cv2.imshow('Select Object', vis_img)
        cv2.setMouseCallback('Select Object', click_handler)

        while True:
            key = cv2.waitKey(100)
            if point:
                break
            if cv2.getWindowProperty('Select Object', cv2.WND_PROP_VISIBLE) < 1:
                print("âŒ çª—å£è¢«å…³é—­ï¼Œæœªè¿›è¡Œç‚¹å‡»")
                return None

        cv2.destroyAllWindows()

        # ä½¿ç”¨ç‚¹å‡»ç‚¹æç¤º SAM
        results = predictor(points=[point], labels=[1])
        center, mask = process_sam_results(results)

    # 6) ä¿å­˜æ©ç 
    if mask is not None:
        cv2.imwrite(output_mask, mask, [cv2.IMWRITE_PNG_BILEVEL, 1])
        print(f"âœ… åˆ†å‰²æ©ç å·²ä¿å­˜ï¼š{output_mask}")
    else:
        print("âš ï¸ åˆ†å‰²å¤±è´¥ï¼Œæœªç”Ÿæˆæ©ç ")

    return mask



if __name__ == '__main__':
    seg_mask = segment_image('color.png')
    print("Segmentation result mask shape:", seg_mask.shape if seg_mask is not None else None)
